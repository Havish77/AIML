{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 592 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'C:/Users/ponmu/OneDrive/Desktop/aiml_ps/New folder/Converted'\n",
    "train_path = 'C:/Users/ponmu/OneDrive/Desktop/aiml_ps/New_Data'\n",
    "ds_train_ = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[224, 224],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Calculate the split sizes\n",
    "total_batches = len(ds_train_)\n",
    "train_split = int(0.90 * total_batches)  # 90% for training\n",
    "\n",
    "# Create training and validation subsets\n",
    "ds_train = ds_train_.take(train_split)  # First 90%\n",
    "ds_valid = ds_train_.skip(train_split)  # Remaining 10%\n",
    "\n",
    "# Optional: Prefetch for performance\n",
    "ds_train = ds_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "ds_valid = ds_valid.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_train = (\n",
    "    ds_train\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "data_valid = (\n",
    "    ds_valid\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ponmu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras.layers' has no attribute 'Droupout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m model_2\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m     14\u001b[0m model_2\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m512\u001b[39m,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 15\u001b[0m model_2\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDroupout(\u001b[38;5;241m0.3\u001b[39m))\n\u001b[0;32m     16\u001b[0m model_2\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBatchNormalization())\n\u001b[0;32m     17\u001b[0m model_2\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras.layers' has no attribute 'Droupout'"
     ]
    }
   ],
   "source": [
    "model_2 = tf.keras.Sequential()\n",
    "# Convolutional layers\n",
    "model_2.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(224,224, 3)))\n",
    "model_2.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_2.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "model_2.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_2.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model_2.add(tf.keras.layers.Flatten())\n",
    "model_2.add(tf.keras.layers.Dense(1024,activation='relu'))\n",
    "model_2.add(tf.keras.layers.Dropout(0.5))\n",
    "model_2.add(tf.keras.layers.Dense(512,activation='relu'))\n",
    "model_2.add(tf.keras.layers.Droupout(0.3))\n",
    "model_2.add(tf.keras.layers.BatchNormalization())\n",
    "model_2.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_2.fit(data_train,validation_data=data_valid,epochs=9,verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_2\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_3.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_2' is not defined"
     ]
    }
   ],
   "source": [
    "model_2.save('model_4.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TestImageDataset:\n",
    "    def __init__(self, image_paths, transform=None):  # Corrected __init__ method\n",
    "        self.image_paths = [path for path in image_paths if self.is_valid_image(path)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def is_valid_image(self, img_path):\n",
    "        try:\n",
    "            Image.open(img_path).convert(\"RGB\")\n",
    "            return True\n",
    "        except UnidentifiedImageError:\n",
    "            print(f\"Skipping invalid image: {img_path}\")\n",
    "            return False\n",
    "\n",
    "    def __len__(self):  # Corrected __len__ method\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):  # Corrected __getitem__ method\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = keras_image.load_img(img_path, target_size=(150, 150))  # Adjust target size as needed\n",
    "        image_array = keras_image.img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_array = self.transform(image_array)\n",
    "\n",
    "        return image_array, os.path.basename(img_path)\n",
    "\n",
    "\n",
    "\n",
    "folder_path='C:/Users/ponmu/OneDrive/Desktop/aiml_ps/Test_Images'\n",
    "image_paths = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path)]\n",
    "#folder_path_2 = 'C:/Users/ponmu/OneDrive/Desktop/aiml_ps/Test_Images'\n",
    "#image_paths_2 = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path_2)]\n",
    "# Load the test dataset\n",
    "test_dataset = TestImageDataset(image_paths)\n",
    "#test_dataset_2 = TestImageDataset(image_paths_2)\n",
    "# Convert to TensorFlow Dataset\n",
    "def generator():\n",
    "    for idx in range(len(test_dataset)):\n",
    "        yield test_dataset[idx]\n",
    "\n",
    "test_tf_dataset = tf.data.Dataset.from_generator(\n",
    "    generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(150, 150, 3), dtype=tf.float32),  # Adjust shape as needed\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "    )\n",
    ").batch(32)\n",
    "\n",
    "# Load your trained model\n",
    "model_path = r'C:\\Users\\ponmu\\OneDrive\\Desktop\\aiml_ps\\the_saga_6.keras'\n",
    "model = load_model(model_path)\n",
    "# Evaluate the model and make predictions\n",
    "predictions = []\n",
    "\n",
    "for batch_images, filenames in test_tf_dataset:\n",
    "    outputs = model(batch_images, training=False)\n",
    "    for filename, output in zip(filenames.numpy(), outputs.numpy()):\n",
    "        label = 1 if output >= 0.5 else 0  # Sigmoid output for binary classification\n",
    "        label_str = 'Real' if label == 1 else 'AI'\n",
    "        predictions.append((filename.decode(\"utf-8\"), label_str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: saga_6.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for prediction in processed_predictions:\\n    print(prediction)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#processed_predictions = [(\"image_\" + str(int(filename.split('_')[1].split('.')[0])), label) for filename, label in predictions]\n",
    "#processed_predictions.sort()\n",
    "processed_predictions = sorted(\n",
    "    [(\"image_\" + str(int(filename.split('_')[1].split('.')[0])), label) for filename, label in predictions],\n",
    "    key=lambda x: int(x[0].split('_')[1])\n",
    ")\n",
    "# Save to CSV file\n",
    "csv_path = 'saga_6.csv'  # Save path can be adjusted\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id', 'Label'])  # Write header\n",
    "    writer.writerows(processed_predictions)  # Write data\n",
    "\n",
    "# Print CSV location\n",
    "print(f\"Predictions saved to: {csv_path}\")\n",
    "\n",
    "# Print predictions for confirmation\n",
    "for prediction in processed_predictions:\n",
    "    print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (2, 1), indices imply (2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example data (you would replace this with your actual dataframe)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m prediction  \u001b[38;5;66;03m# Add all your data here\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Modify the 'Index' column to show 'image_i'\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ponmu\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    859\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m             arrays,\n\u001b[0;32m    861\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    868\u001b[0m             data,\n\u001b[0;32m    869\u001b[0m             index,\n\u001b[0;32m    870\u001b[0m             columns,\n\u001b[0;32m    871\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    872\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    873\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    874\u001b[0m         )\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    877\u001b[0m         {},\n\u001b[0;32m    878\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    882\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ponmu\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ponmu\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (2, 1), indices imply (2, 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data (you would replace this with your actual dataframe)\n",
    "data = prediction  # Add all your data here\n",
    "df = pd.DataFrame(data, columns=['Index', 'Category'])\n",
    "\n",
    "# Modify the 'Index' column to show 'image_i'\n",
    "df['Index'] = df['Index'].apply(lambda x: f'image_{x}')\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
